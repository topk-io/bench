{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!uv pip install polars\n",
        "import polars as pl\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ANONYMIZE = True\n",
        "ANONYMIZE_SEED = 1\n",
        "DROP_THE_WORST = True\n",
        "AGGREGATE_RUNS = True\n",
        "EXPORT_JSON = True\n",
        "EXPORT_PATH = \"./topk-bench-results\"\n",
        "\n",
        "# List of files (produced by `tb.write_metrics()`) for `mode=ingest`\n",
        "INGEST_FILES = []\n",
        "# List of files (produced by `tb.write_metrics()`) for `mode=qps`, `mode=filter`, `mode=rw`\n",
        "QUERY_FILES = []\n",
        "\n",
        "ingest_df = pl.concat([pl.read_parquet(file) for file in INGEST_FILES]) \n",
        "ingest_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import builtins\n",
        "\n",
        "def fmt_int_filter(v):\n",
        "    if not v:\n",
        "        return \"\"\n",
        "    return f\"int {builtins.int(v) // 100}%\"\n",
        "\n",
        "def fmt_keyword_filter(v):\n",
        "    if not v:\n",
        "        return \"\"\n",
        "    return f\"keyword {builtins.int(v.lstrip('0')) // 100}%\"\n",
        "\n",
        "def fmt_selectivity(row):\n",
        "    keyword_filter = row[\"keyword_filter\"]\n",
        "    int_filter = row[\"int_filter\"]\n",
        "\n",
        "    if keyword_filter == '' and int_filter == '':\n",
        "        return 'unfiltered'\n",
        "    \n",
        "    if keyword_filter == '':\n",
        "        return int_filter\n",
        "    \n",
        "    if int_filter == '':\n",
        "        return keyword_filter\n",
        "\n",
        "    return f\"{int_filter} {keyword_filter}\"\n",
        "\n",
        "def filter_by_type(filter_type, unfiltered=True):\n",
        "    cond = None\n",
        "\n",
        "    if unfiltered:\n",
        "        cond = (pl.col(\"keyword_filter\") == \"\") & (pl.col(\"int_filter\") == \"\")\n",
        "\n",
        "    if filter_type == \"keyword_filter\":\n",
        "        cond = cond | (pl.col(\"keyword_filter\") != \"\")\n",
        "    elif filter_type == \"int_filter\":\n",
        "        cond = cond | (pl.col(\"int_filter\") != \"\")\n",
        "\n",
        "    return cond\n",
        "\n",
        "def anonymize_providers(df):\n",
        "    providers = set(df['provider'].unique().sort().to_list()) - {\"topk\"}\n",
        "\n",
        "    anon_providers = [f\"Provider {chr(i)}\" for i in range(ord('A'), ord('A') + len(providers))]\n",
        "    rng = random.Random(ANONYMIZE_SEED)\n",
        "    rng.shuffle(anon_providers)\n",
        "\n",
        "    mapping = dict(zip(providers, anon_providers))\n",
        "    mapping = { \"topk\": \"TopK\", **mapping }\n",
        "\n",
        "    return df.with_columns(pl.col(\"provider\").replace(mapping).alias(\"provider\"))\n",
        "\n",
        "\n",
        "def drop_worst(df, group_by, ascending=False):\n",
        "    agg_group_by = [col for col in group_by if col != \"run_id\"]\n",
        "    df_sorted = df.sort(by=agg_group_by + [\"value\"], descending=not ascending)\n",
        "    df_sorted = df_sorted.with_row_index(\"row_idx\")\n",
        "    max_idx_per_group = (\n",
        "        df_sorted.group_by(agg_group_by)\n",
        "        .agg([pl.col(\"row_idx\").max().alias(\"max_row_idx\")])\n",
        "    )\n",
        "    df_sorted = df_sorted.join(max_idx_per_group, on=agg_group_by)\n",
        "    df_result = df_sorted.filter(pl.col(\"row_idx\") != pl.col(\"max_row_idx\")).drop([\"row_idx\", \"max_row_idx\"])\n",
        "    return df_result\n",
        "\n",
        "\n",
        "def aggregate_runs(df, group_by):\n",
        "    return df.group_by([col for col in group_by if col != \"run_id\"]).agg([\n",
        "        pl.col(\"value\").mean().alias(\"value\"),\n",
        "        pl.col(\"value\").min().alias(\"value_min\"),\n",
        "        pl.col(\"value\").max().alias(\"value_max\"),\n",
        "    ])\n",
        "\n",
        "def process_df(df, group_by, asc):\n",
        "    if DROP_THE_WORST:\n",
        "        df = drop_worst(df, group_by, asc)\n",
        "\n",
        "    if AGGREGATE_RUNS:\n",
        "        df = aggregate_runs(df, group_by)\n",
        "\n",
        "    if ANONYMIZE:\n",
        "        df = anonymize_providers(df)\n",
        "\n",
        "    return df.sort(\"provider\")\n",
        "\n",
        "def write_df(df, filename):\n",
        "    filename = filename.replace(\"/\", \"_\").replace(\" \", \"_\").replace(\":\", \"_\")\n",
        "    path = f\"{EXPORT_PATH}/{filename}\"\n",
        "    df.write_json(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_bars(df, **kwargs):\n",
        "    labels = kwargs.pop('labels', {})   \n",
        "    y = kwargs.pop('y', \"value\")\n",
        "    \n",
        "    if ANONYMIZE:\n",
        "        provider_order = [\"TopK\", \"Provider A\", \"Provider B\", \"Provider C\", \"Provider D\"]\n",
        "    else:\n",
        "        providers = df[\"provider\"].unique().sort().to_list()\n",
        "        if \"topk\" in providers:\n",
        "            providers.remove(\"topk\")\n",
        "            providers = [\"topk\"] + providers\n",
        "            \n",
        "        provider_order = providers\n",
        "\n",
        "    fig = px.bar(\n",
        "        df,\n",
        "        barmode=\"group\",\n",
        "        labels={\n",
        "            \"provider\": \"Provider\",\n",
        "            **labels,\n",
        "        },\n",
        "        category_orders={\n",
        "            \"size\": [\"100k\", \"1m\", \"10m\"],\n",
        "            \"concurrency\": [1, 2, 4, 8],\n",
        "            \"provider\": provider_order,\n",
        "            \"keyword_filter\": [\"keyword 100%\", \"keyword 10%\", \"keyword 1%\"],\n",
        "            \"int_filter\": [\"int 100%\", \"int 10%\", \"int 1%\"],\n",
        "            \"selectivity\": [\"unfiltered\", \"int 100%\", \"int 10%\", \"int 1%\", \"keyword 100%\", \"keyword 10%\", \"keyword 1%\"],\n",
        "            \"read_write\": [\"false\", \"true\"],\n",
        "        },\n",
        "        y=y,\n",
        "        **kwargs,\n",
        "    )\n",
        "    return fig\n",
        "\n",
        "def plot_latency(df, x, color, title):\n",
        "    group_by = set([\"provider\", x, color, \"size\", \"run_id\"])\n",
        "\n",
        "    latency_df = (\n",
        "        df\n",
        "        .filter(pl.col(\"metric\") == \"bench.query.latency_ms\")\n",
        "        .group_by(group_by)\n",
        "        .agg([\n",
        "            pl.col(\"value\").quantile(0.99).alias(\"value\")\n",
        "        ])\n",
        "    )\n",
        "    latency_df = process_df(latency_df, group_by, asc=True)\n",
        "\n",
        "    fig = plot_bars(\n",
        "        latency_df,\n",
        "        x=x,\n",
        "        color=color,\n",
        "        facet_col=\"size\",\n",
        "        title=f\"{title}: p99 latency\",\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "    if EXPORT_JSON:\n",
        "        write_df(latency_df, f\"{title}_latency.json\")\n",
        "\n",
        "def plot_qps(df, x, color, title):\n",
        "    group_by = set([\"provider\", x, color, \"size\", \"run_id\"])\n",
        "    qps_df = (\n",
        "        df\n",
        "        .filter(pl.col(\"metric\") == \"bench.query.latency_ms\") \n",
        "        .group_by(group_by)\n",
        "        .agg([\n",
        "            pl.col(\"ts\").min().alias(\"start\"),\n",
        "            pl.col(\"ts\").max().alias(\"end\"),\n",
        "            pl.len().alias(\"n_requests\"),\n",
        "        ])\n",
        "        .with_columns(\n",
        "            (pl.col(\"n_requests\") / ((pl.col(\"end\") - pl.col(\"start\")).dt.total_seconds())).alias(\"value\")\n",
        "        )\n",
        "    )\n",
        "    qps_df = process_df(qps_df, group_by, asc=False)\n",
        "    \n",
        "    fig = plot_bars(\n",
        "        qps_df,\n",
        "        x=x,\n",
        "        color=color,\n",
        "        facet_col=\"size\",\n",
        "        title=f\"{title}: QPS\",\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "    if EXPORT_JSON:\n",
        "        write_df(qps_df, f\"{title}_qps.json\")\n",
        "\n",
        "def plot_recall(df, x, color, title):\n",
        "    group_by = set([\"provider\", x, color, \"size\", \"run_id\"])    \n",
        "    recall_df = (\n",
        "        df\n",
        "        .filter(pl.col(\"metric\") == \"bench.query.recall\")\n",
        "        .group_by(group_by)\n",
        "        .agg([\n",
        "            pl.col(\"value\").mean().alias(\"value\"),\n",
        "        ])\n",
        "    )\n",
        "    recall_df = process_df(recall_df, group_by, asc=False)\n",
        "\n",
        "    fig = plot_bars(\n",
        "        recall_df,\n",
        "        x=x,\n",
        "        color=color,\n",
        "        facet_col=\"size\",\n",
        "        title=f\"{title}: recall (avg)\",\n",
        "    )\n",
        "    fig.update_yaxes(range=[\n",
        "        recall_df['value'].min() - 0.01,\n",
        "        recall_df['value'].max() + 0.01,\n",
        "    ])\n",
        "    fig.show()\n",
        "\n",
        "    if EXPORT_JSON:\n",
        "        write_df(recall_df, f\"{title}_recall.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pl.concat([pl.read_parquet(p) for p in QUERY_FILES]) \n",
        "df = df.filter(pl.col('warmup') == 'false') # exclude warmups\n",
        "df = df.with_columns(\n",
        "    pl.col(\"int_filter\").map_elements(fmt_int_filter, return_dtype=pl.String).alias(\"int_filter\"),\n",
        "    pl.col(\"keyword_filter\").map_elements(fmt_keyword_filter, return_dtype=pl.String).alias(\"keyword_filter\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "latency_df = (\n",
        "    ingest_df\n",
        "    .filter(pl.col(\"metric\") == \"bench.ingest.latency_ms\")\n",
        "    .group_by([\"provider\", \"size\", \"run_id\"])\n",
        "    .agg([\n",
        "        pl.col(\"ts\").min().alias(\"start_ts\"),\n",
        "        pl.col(\"ts\").max().alias(\"end_ts\"),\n",
        "    ])\n",
        "    .with_columns([\n",
        "        (pl.col(\"end_ts\") - pl.col(\"start_ts\")).dt.total_seconds(fractional=True).alias(\"value\")\n",
        "    ])\n",
        ")\n",
        "latency_df = anonymize_providers(latency_df)\n",
        "\n",
        "fig = px.bar(\n",
        "    latency_df.sort(\"value\"),\n",
        "    x=\"value\",\n",
        "    y=\"provider\",\n",
        "    color=\"provider\",\n",
        "    facet_col=\"size\",\n",
        "    title=\"Total Ingest Latency (s)\",\n",
        "    category_orders={\n",
        "        \"size\": [\"100k\", \"1m\", \"10m\"],\n",
        "    },\n",
        ")\n",
        "fig.update_xaxes(matches=None)\n",
        "fig.show()\n",
        "\n",
        "write_df(latency_df, \"ingest_latency.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "throughput_df = (\n",
        "    ingest_df\n",
        "    .filter(pl.col(\"metric\") == \"bench.ingest.upserted_bytes\")\n",
        "    .group_by([\"provider\", \"size\", \"run_id\"])\n",
        "    .agg([\n",
        "        pl.col(\"value\").sum().alias(\"total_bytes\"),\n",
        "        pl.col(\"ts\").min().alias(\"start_ts\"),\n",
        "        pl.col(\"ts\").max().alias(\"end_ts\"),\n",
        "    ])\n",
        "    .with_columns([\n",
        "        (pl.col(\"total_bytes\") / (pl.col(\"end_ts\") - pl.col(\"start_ts\")).dt.total_seconds(fractional=True)).alias(\"value\")\n",
        "    ])\n",
        ")\n",
        "throughput_df = anonymize_providers(throughput_df)\n",
        "\n",
        "fig = px.bar(\n",
        "    throughput_df.sort(\"value\", descending=True),\n",
        "    y=\"provider\",\n",
        "    x=\"value\",\n",
        "    color=\"provider\",\n",
        "    facet_col=\"size\",\n",
        "    title=\"Ingest Throughput (bytes/sec)\",\n",
        "    labels={\"value\": \"Avg Throughput (bytes/sec)\"},\n",
        "    category_orders={\n",
        "        \"size\": [\"100k\", \"1m\", \"10m\"],\n",
        "    },\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "write_df(throughput_df, \"ingest_throughput.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "freshness_df = (\n",
        "    ingest_df\n",
        "    .filter(pl.col(\"metric\") == \"bench.ingest.freshness_latency_ms\")\n",
        "    .sort(\"ts\")\n",
        "    .group_by([\"provider\", \"size\", \"run_id\"])\n",
        "    .agg([\n",
        "        pl.col(\"value\").quantile(0.99).alias(\"p99\"),\n",
        "        pl.col(\"value\").quantile(0.90).alias(\"p90\"),\n",
        "        pl.col(\"value\").quantile(0.50).alias(\"p50\"),\n",
        "    ])\n",
        "    .unpivot(\n",
        "        index=[\"provider\", \"size\", \"run_id\"],\n",
        "        on=[\"p99\", \"p90\", \"p50\"],\n",
        "        variable_name=\"quantile\",\n",
        "        value_name=\"value\"\n",
        "    )\n",
        ")\n",
        "freshness_df = anonymize_providers(freshness_df)\n",
        "\n",
        "fig = plot_bars(\n",
        "    freshness_df,\n",
        "    x=\"provider\",\n",
        "    color=\"quantile\",\n",
        "    facet_col=\"size\",\n",
        "    title=\"Freshness Quantiles (p99, p90, p50)\",\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "write_df(freshness_df, \"ingest_freshness.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "concurrency_df = df.filter(pl.col(\"mode\") == \"qps\")\n",
        "\n",
        "concurrency_latency_df = plot_latency(concurrency_df, x=\"concurrency\", color=\"provider\", title=\"concurrency\")\n",
        "concurrency_qps_df = plot_qps(concurrency_df, x=\"concurrency\", color=\"provider\", title=\"concurrency\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "overhead_df = (\n",
        "    df\n",
        "    .filter(pl.col(\"mode\") == \"filter\")\n",
        "    .filter(\n",
        "        ((pl.col(\"keyword_filter\") == \"\") & (pl.col(\"int_filter\") == \"\"))\n",
        "        | ((pl.col(\"keyword_filter\") == \"keyword 100%\") & (pl.col(\"int_filter\") == \"\"))\n",
        "        | ((pl.col(\"keyword_filter\") == \"\") & (pl.col(\"int_filter\") == \"int 100%\"))\n",
        "    )\n",
        "    .with_columns(\n",
        "        pl.struct([pl.col(\"keyword_filter\"), pl.col(\"int_filter\")])\n",
        "        .map_elements(fmt_selectivity, return_dtype=pl.String)\n",
        "        .alias(\"selectivity\")\n",
        "    )\n",
        ")\n",
        "\n",
        "plot_latency(overhead_df, x=\"provider\", color=\"selectivity\", title=\"filtering overhead\")\n",
        "plot_qps(overhead_df, x=\"provider\", color=\"selectivity\", title=\"filtering overhead\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_df = (\n",
        "    df\n",
        "    .filter(pl.col(\"mode\") == \"filter\")\n",
        "    .filter(filter_by_type(\"int_filter\"))\n",
        ")\n",
        "\n",
        "plot_latency(base_df, x=\"provider\", color=\"int_filter\", title=\"int_filter\")\n",
        "plot_qps(base_df, x=\"provider\", color=\"int_filter\", title=\"int_filter\")\n",
        "plot_recall(base_df, x=\"provider\", color=\"int_filter\", title=\"int_filter\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_df = (\n",
        "    df\n",
        "    .filter(pl.col(\"mode\") == \"filter\")\n",
        "    .filter(filter_by_type(\"keyword_filter\"))\n",
        ")\n",
        "\n",
        "plot_latency(base_df, x=\"provider\", color=\"keyword_filter\", title=\"keyword_filter\")\n",
        "plot_qps(base_df, x=\"provider\", color=\"keyword_filter\", title=\"keyword_filter\")\n",
        "plot_recall(base_df, x=\"provider\", color=\"keyword_filter\", title=\"keyword_filter\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rw_df = df.filter(pl.col(\"mode\") == \"rw\")\n",
        "\n",
        "plot_latency(rw_df, x=\"provider\", color=\"read_write\", title=\"read/write\")\n",
        "plot_qps(rw_df, x=\"provider\", color=\"read_write\", title=\"read/write\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
